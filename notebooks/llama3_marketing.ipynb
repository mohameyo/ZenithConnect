{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "67e356f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7861\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7861/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IMPORTANT: You are using gradio version 3.50.2, however version 4.29.0 is available, please upgrade.\n",
      "--------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-20 22:34:40,021 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-05-20 22:34:41,476 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-05-20 22:35:53,284 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-05-20 22:35:54,689 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-05-20 22:36:17,635 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-05-20 22:36:46,716 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-05-20 22:36:46,718 - INFO - Providing marketing advice.\n",
      "2024-05-20 22:36:48,975 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-05-20 22:37:42,689 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-05-20 22:37:42,692 - INFO - Providing marketing advice.\n",
      "2024-05-20 22:37:44,060 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-05-20 22:38:16,358 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\mrahman\\AppData\\Local\\anaconda3\\Lib\\site-packages\\gradio\\queueing.py\", line 407, in call_prediction\n",
      "    output = await route_utils.call_process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\mrahman\\AppData\\Local\\anaconda3\\Lib\\site-packages\\gradio\\route_utils.py\", line 226, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\mrahman\\AppData\\Local\\anaconda3\\Lib\\site-packages\\gradio\\blocks.py\", line 1550, in process_api\n",
      "    result = await self.call_function(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\mrahman\\AppData\\Local\\anaconda3\\Lib\\site-packages\\gradio\\blocks.py\", line 1199, in call_function\n",
      "    prediction = await utils.async_iteration(iterator)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\mrahman\\AppData\\Local\\anaconda3\\Lib\\site-packages\\gradio\\utils.py\", line 519, in async_iteration\n",
      "    return await iterator.__anext__()\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\mrahman\\AppData\\Local\\anaconda3\\Lib\\site-packages\\gradio\\utils.py\", line 623, in asyncgen_wrapper\n",
      "    async for response in f(*args, **kwargs):\n",
      "  File \"C:\\Users\\mrahman\\AppData\\Local\\anaconda3\\Lib\\site-packages\\gradio\\chat_interface.py\", line 437, in _stream_fn\n",
      "    first_response = await async_iteration(generator)\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\mrahman\\AppData\\Local\\anaconda3\\Lib\\site-packages\\gradio\\utils.py\", line 519, in async_iteration\n",
      "    return await iterator.__anext__()\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\mrahman\\AppData\\Local\\anaconda3\\Lib\\site-packages\\gradio\\utils.py\", line 512, in __anext__\n",
      "    return await anyio.to_thread.run_sync(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\mrahman\\AppData\\Local\\anaconda3\\Lib\\site-packages\\anyio\\to_thread.py\", line 33, in run_sync\n",
      "    return await get_asynclib().run_sync_in_worker_thread(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\mrahman\\AppData\\Local\\anaconda3\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 877, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\mrahman\\AppData\\Local\\anaconda3\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 807, in run\n",
      "    result = context.run(func, *args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\mrahman\\AppData\\Local\\anaconda3\\Lib\\site-packages\\gradio\\utils.py\", line 495, in run_sync_iterator_async\n",
      "    return next(iterator)\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\mrahman\\AppData\\Local\\Temp\\ipykernel_22416\\4072503690.py\", line 180, in predict\n",
      "    for character in gpt_response[\"output\"]:\n",
      "TypeError: 'NoneType' object is not iterable\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "import json\n",
    "from groq import Groq\n",
    "import requests\n",
    "import gradio as gr\n",
    "import time\n",
    "import logging\n",
    "from langchain.schema import AIMessage, HumanMessage\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Initialize OpenAI client\n",
    "client = Groq(api_key = \"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\")\n",
    "MODEL = 'llama3-70b-8192'\n",
    "\n",
    "# Define the churn prediction function\n",
    "def predict_churn(data: dict) -> dict:\n",
    "    url = 'http://127.0.0.1:5000/predict'\n",
    "    try:\n",
    "        response = requests.post(url, json=data)\n",
    "        response.raise_for_status()  # Raise HTTPError for bad responses\n",
    "        return response.json()\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        logging.error(f\"Request failed: {e}\")\n",
    "        return {\n",
    "            \"error\": f\"Request failed: {str(e)}\",\n",
    "            \"details\": response.text if response else \"No response text available\"\n",
    "        }\n",
    "\n",
    "# Initialize chat history\n",
    "chat_history = []\n",
    "\n",
    "# Define the provide_marketing_advice function\n",
    "def provide_marketing_advice(data: dict) -> dict:\n",
    "    # This is a placeholder function that returns a generic marketing advice response\n",
    "    # In a real-world scenario, I would implement actual logic to provide advice maybre RAG arch or fine-tuned model.\n",
    "    logging.info(\"Providing marketing advice.\")\n",
    "    return {\n",
    "        \"advice\": \"Based on the latest trends in the telecom industry in Saudi, we recommend focusing on personalized customer experiences, improving customer service, and offering competitive pricing and promotions to retain customers.\"\n",
    "    }\n",
    "\n",
    "# Define the conversation function\n",
    "def run_conversation(message):\n",
    "    global chat_history\n",
    "\n",
    "    # General marketing prompt\n",
    "    pre_prompt = \"\"\"\n",
    "    You are a marketing assistant. You work for Zenith Connect. Your role is to assist the marketing team by providing data-driven insights to help understand customer behavior and improve customer retention. You can answer questions about customer churn prediction, provide marketing advice, greet the team, or handle any general marketing-related queries. Make sure to provide personalized and helpful responses.\n",
    "\n",
    "    For customer churn prediction, the marketing team can provide:\n",
    "    - customerID:\n",
    "    - gender: Male, Female\n",
    "    - SeniorCitizen: 0, 1\n",
    "    - Partner: Yes, No\n",
    "    - Dependents: Yes, No\n",
    "    - tenure: integer (number of months) \n",
    "    - PhoneService: Yes, No\n",
    "    - MultipleLines: Yes, No, No phone service\n",
    "    - InternetService: DSL, Fiber optic, No\n",
    "    - OnlineSecurity: Yes, No, No internet service\n",
    "    - OnlineBackup: Yes, No, No internet service\n",
    "    - DeviceProtection: Yes, No, No internet service\n",
    "    - TechSupport: Yes, No, No internet service\n",
    "    - StreamingTV: Yes, No, No internet service\n",
    "    - StreamingMovies: Yes, No, No internet service\n",
    "    - Contract: Month-to-month, One year, Two year\n",
    "    - PaperlessBilling: Yes, No\n",
    "    - PaymentMethod: Electronic check, Mailed check, Bank transfer (automatic), Credit card (automatic) (default = Electronic check)\n",
    "    - MonthlyCharges: Float (monthly charge amount)\n",
    "    - TotalCharges: Float (total charge amount)\n",
    "    - Churn: Yes, No\n",
    "\n",
    "    If the user is asking about customer churn, output the data as a JSON object to be used as input for a machine learning model. Otherwise, provide general marketing advice or handle any other marketing-related queries.\n",
    "    # Take all information for the model .. but when communicate back just tell them what they were considered about with reason why churn.\n",
    "    \"\"\"\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": pre_prompt},\n",
    "        {\"role\": \"user\", \"content\": message['input']}\n",
    "    ]\n",
    "\n",
    "    # Define tools\n",
    "    tools = [\n",
    "        {\n",
    "            \"type\": \"function\",\n",
    "            \"function\": {\n",
    "                \"name\": \"predict_churn\",\n",
    "                \"description\": \"Get the churn score and prediction\",\n",
    "                \"parameters\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"data\": {\n",
    "                            \"type\": \"object\",\n",
    "                            \"description\": \"The customer data for churn prediction\",\n",
    "                        }\n",
    "                    },\n",
    "                    \"required\": [\"data\"]\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            \"type\": \"function\",\n",
    "            \"function\": {\n",
    "                \"name\": \"provide_marketing_advice\",\n",
    "                \"description\": \"Provide general marketing advice\",\n",
    "                \"parameters\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"data\": {\n",
    "                            \"type\": \"object\",\n",
    "                            \"description\": \"Any additional data needed for advice\",\n",
    "                        }\n",
    "                    },\n",
    "                    \"required\": [\"data\"]\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=MODEL,\n",
    "            messages=messages,\n",
    "            tools=tools,\n",
    "            tool_choice=\"auto\",\n",
    "            max_tokens=8000\n",
    "        )\n",
    "\n",
    "        response_message = response.choices[0].message\n",
    "        tool_calls = getattr(response_message, 'tool_calls', [])\n",
    "\n",
    "        if tool_calls:\n",
    "            available_functions = {\n",
    "                \"predict_churn\": predict_churn,\n",
    "                \"provide_marketing_advice\": provide_marketing_advice,\n",
    "            }\n",
    "            messages.append(response_message)\n",
    "\n",
    "            for tool_call in tool_calls:\n",
    "                function_name = tool_call.function.name\n",
    "                function_to_call = available_functions[function_name]\n",
    "                function_args = json.loads(tool_call.function.arguments)\n",
    "                function_response = function_to_call(data=function_args.get(\"data\"))\n",
    "                messages.append({\n",
    "                    \"tool_call_id\": tool_call.id,\n",
    "                    \"role\": \"tool\",\n",
    "                    \"name\": function_name,\n",
    "                    \"content\": json.dumps(function_response)\n",
    "                })\n",
    "            \n",
    "            second_response = client.chat.completions.create(\n",
    "                model=MODEL,\n",
    "                messages=messages\n",
    "            )\n",
    "            chat_history.append((message['input'], second_response.choices[0].message.content))\n",
    "            return {\"output\": second_response.choices[0].message.content}\n",
    "        else:\n",
    "            chat_history.append((message['input'], response_message.content))\n",
    "            return {\"output\": response_message.content}\n",
    "    except Exception as e:\n",
    "        logging.error(f\"An error occurred during conversation: {e}\")\n",
    "        return {\"output\": f\"An error occurred: {str(e)}\"}\n",
    "\n",
    "# Function to handle prediction and conversation\n",
    "def predict(message, history):\n",
    "    history_langchain_format = []\n",
    "    for human, ai in history:\n",
    "        history_langchain_format.append(HumanMessage(content=human))\n",
    "        history_langchain_format.append(AIMessage(content=ai))\n",
    "    history_langchain_format.append(HumanMessage(content=message))\n",
    "    gpt_response = run_conversation({\"input\": message})\n",
    "    \n",
    "    response_message = \"\"\n",
    "    for character in gpt_response[\"output\"]:\n",
    "        response_message += character\n",
    "        time.sleep(0.01)\n",
    "        yield response_message\n",
    "\n",
    "# Create a Gradio interface\n",
    "theme = gr.themes.Base(primary_hue=\"green\")\n",
    "chat_interface = gr.ChatInterface(predict, autofocus=False, theme=theme)\n",
    "\n",
    "# Enable the queue\n",
    "chat_interface.queue()\n",
    "\n",
    "# Launch the Gradio interface\n",
    "chat_interface.launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
